<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction">
  <meta name="keywords" content="MonoSDF, Neural Implicit Surface Reconstruction, Monocular Geometric Cues ">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>MonoSDF</title>

  <!-- Bootstrap -->
  <link href="static/css/bootstrap-4.4.1.css" rel="stylesheet">
  
  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <script type="module" src="https://unpkg.com/@google/model-viewer/dist/model-viewer.min.js"></script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/eye.png">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title" style="margin-bottom: 0"><strong>MonoSDF</strong></h1>
          <h2 class="title is-2 publication-title" style="margin-top: 0; margin-bottom: 0">Exploring Monocular Geometric Cues for </h2>
          <h2 class="title is-2 publication-title" style="margin-top: 0">Neural Implicit Surface Reconstruction</h2>
          <div class="column is-full_width">
            <h2 class="title is-4">NeurIPS 2022</h2>
          </div>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://niujinshuchong.github.io/">Zehao Yu</a><sup>1</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://pengsongyou.github.io/">Songyou Peng</a><sup>2,3</sup></span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://m-niemeyer.github.io/">Michael Niemeyer</a><sup>1,3</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="https://tsattler.github.io/">Torsten Sattler</a><sup>4</sup>
            </span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block">
              <a href="http://www.cvlibs.net/">Andreas Geiger</a><sup>1,3</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>University of Tübingen</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>2</sup>ETH Zurich</span>&nbsp;&nbsp;&nbsp;&nbsp;
            <span class="author-block"><sup>3</sup>MPI for Intelligent Systems, Tübingen</span>
            <span class="author-block"><sup>4</sup>Czech Technical University in Prague</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://arxiv.org/pdf/2206.00665.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <!--
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              -->
              <!-- Video Link. -->
              <!--
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/autonomousvision/monosdf"
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>

              <!-- Code Link. -->
              <span class="link-block">
                <a href="https://github.com/autonomousvision/sdfstudio"
                   class="external-link button is-normal is-rounded is-dark disabled">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>SDFStudio</span>
                  </a>
              </span>

              <!-- Dataset Link. -->
              <!--
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span>
              -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./resources/teaser_with_dataset.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered" style="margin-top: 15px">
        TL;DR: We demonstrate that state-of-the-art depth and normal cues extracted from monocular images are complementary to reconstruction cues and hence significantly improve the performance of implicit surface reconstruction methods.
        <!-- TL;DR: <span class="dnerf">MonoSDF</span> explore the utility of monocular geometric cues for improving neural implicit surface reconstruction. -->
      </h2>
    </div>
  </div>
</section>

<!--
<section class="section">
  <div class="container is-max-desktop">

    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Reconstructions</h2>

        <div class="embed-responsive embed-responsive-16by9">

          <iframe style="clip-path: inset(1px 1px)" src="https://sketchfab.com/playlists/embed?collection=abee3cc1a7a7436c804f2bd3aadc2acd" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true" width="100%" height="100%" frameborder="0"></iframe>
        </div>
        

      </div>
    </div>

  </div>
</section>

-->

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3" style="margin-top: -30px">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In recent years, neural implicit surface reconstruction methods have become popular for multi-view 3D reconstruction.
            In contrast to traditional multi-view stereo methods, these approaches tend to produce smoother and more complete reconstructions due to the inductive smoothness bias of neural networks.
            State-of-the-art neural implicit methods allow for high-quality reconstructions of simple scenes from many input views. 
            Yet, their performance drops significantly for larger and more complex scenes and scenes captured from sparse viewpoints.
            This is caused primarily by the inherent ambiguity in the RGB reconstruction loss that does not provide enough constraints, in particular in less-observed and textureless areas. 
            Motivated by recent advances in the area of monocular geometry prediction, we systematically explore the utility these cues provide for improving neural implicit surface reconstruction. 
            We demonstrate that depth and normal cues, predicted by general-purpose monocular estimators, significantly improve reconstruction quality and optimization time.
            Further, we analyse and investigate multiple design choices for representing neural implicit surfaces, ranging from monolithic MLP models over single-grid to multi-resolution grid representations.
            We observe that geometric monocular priors improve performance both for small-scale single-object as well as large-scale multi-object scenes, independent of the choice of representation.  
          </p>
        </div>
      </div>
    </div>
    <!--/ Abstract. -->
    <!-- Paper video. -->
    <!--
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div>
    -->
    <!--/ Paper video. -->
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">

    <!-- Method. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-full-width">
        <h2 class="title is-3" style="margin-top: -20px">Method</h2>

        <img src="./resources/method.svg" class="center">
        <div class="content has-text-justified">
          <p style="margin-top: 30px">
            We use monocular geometric cues predicted by a general-purpose pretrained network to guide the optimization of neural implicit surface models. 
            More specifically, for a batch of rays, we volume render predicted RGB colors, depth, and normals, and optimize wrt. the input RGB images and monocular geometric cues. 
            Further, we investigate different design choices for neural implicit architectures and provide an in-depth analysis. 
          </p>
        </div>

      </div>
    </div>
    <!--/ Animation. -->

  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">

    <!-- Results. -->
    <div class="columns is-centered ">
      <div class="column is-full-width">
        <h2 class="title is-3 has-text-centered" style="margin-top: -30px">Results</h2>


        <h3 class="title is-4">ScanNet</h3>
        <div class="content has-text-justified">
          <p>
            We test our method on the ScanNet dataset and compare to state-of-the-art methods. Our approach achieves significantly better reconstruction results. 
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="90%">
            <source src="./resources/scannet.mp4"
                    type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">Tanks and Temples</h3>
        <div class="content has-text-justified">
          <p>
            We test our method on the Tanks and Temples dataset and compare to state-of-the-art methods. 
            MonoSDF is the first neural implicit model achieving reasonable results on such a large-scale indoor scene.
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="90%">
            <source src="./resources/TNT.mp4"
                    type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">Tanks and Temples with High-resolution Monocular Cues</h3>
        <div class="content has-text-justified">
          <p>
            We show a preliminary result of using high-resolution cues in the Tanks and Temples dataset.
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                 autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="100%">
            <source src="./resources/TNT_highres.mp4"
                    type="video/mp4">
          </video>
        </div>


        <h3 class="title is-4">DTU with <b>3 Input Views</b> </h3>
        <div class="content has-text-justified">
          <p>
            We test our method on the DTU dataset with <strong>only 3 input views</strong>. Our monocular geometric cues significantly boost the reconstruction results. 
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                  autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="90%">
            <source src="./resources/DTU.mp4"
                    type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">DTU with <b>All Input Views</b> </h3>
        <div class="content has-text-justified">
          <p>
            We test our method on the DTU dataset with <strong>all input views</strong>. Using multi-resolution feature grids with monocular geometric cues significantly boost the reconstruction results. 
          </p>
        </div>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                  autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="90%">
            <source src="./resources/DTU_allview.mp4"
                    type="video/mp4">
          </video>
        </div>


        <h3 class="title is-4">Ablation on Replica</h3>
        
        <div class="content has-text-centered">
          <video id="replay-video"
                  autoplay
                 controls
                 muted
                 preload
                 playsinline
                 loop
                 width="90%">
            <source src="./resources/ablation.mp4"
                    type="video/mp4">
          </video>
        </div>

        <h3 class="title is-4">Reconstructions</h3>
        
        <div class="embed-responsive embed-responsive-16by9">

          <iframe style="clip-path: inset(1px 1px)" src="https://sketchfab.com/playlists/embed?autostart=1&amp;autospin=0.25&amp;collection=abee3cc1a7a7436c804f2bd3aadc2acd" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture; fullscreen" mozallowfullscreen="true" webkitallowfullscreen="true" width="100%" height="100%" frameborder="0"></iframe>
        </div>
        

      </div>
    </div>

  </div>
</section>



<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{Yu2022MonoSDF,
  author    = {Yu, Zehao and Peng, Songyou and Niemeyer, Michael and Sattler, Torsten and Geiger, Andreas},
  title     = {MonoSDF: Exploring Monocular Geometric Cues for Neural Implicit Surface Reconstruction},
  journal   = {Advances in Neural Information Processing Systems (NeurIPS)},
  year      = {2022},
}</code></pre>
  </div>
</section>

<section class="section" id="Acknowledgements">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgements</h2>
    This work was supported by an NVIDIA research gift. 
    We thank the Max Planck ETH Center for Learning Systems (CLS) for supporting SP and the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for supporting MN. 
    ZY is supported by BMWi in the project KI Delta Learning (project number 19A19013O). 
    AG is supported by the ERC Starting Grant LEGO-3D (850533) and DFG EXC number 2064/1 - project number 390727645.
    TS is supported by the EU Horizon 2020 project RICAIP (grant agreeement No.857306), and the European Regional Development Fund under project IMPACT (No. CZ.02.1.01/0.0/0.0/15_003/0000468).
    We also thank the authors of Manhattan-SDF for sharing baseline results on ScanNet. We also thank Christian
    Reiser and Zijian Dong for proofreading.
  </div>
</section>


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            This webpage template is from <a href="https://github.com/nerfies/nerfies.github.io">Nerfies</a>. 
            We sincerely thank <a href="https://keunhong.com/">Keunhong Park</a> for developing and open-sourcing this template.
          </p>
        </div>
      </div>
          <p></p>
        </div>
      </div>
</footer>

</body>
</html>
